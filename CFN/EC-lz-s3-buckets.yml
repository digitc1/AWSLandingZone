AWSTemplateFormatVersion: 2010-09-09

#   --------------------------------------------------------
#   Version History
#
#   v1.0.0    J. Vandenbergen   Initial Version
#   v1.1.0    J. Silva          Added SSL Secure Transport Only policy on all buckets
#   --------------------------------------------------------

Description: >-
    v1.0. Creates S3 buckets for CloudTrail, Config and Access Logs.
    Enables versioning for CloudTrail and Config buckets.
    CloudTrail and Config Buckets push access logs to Access Logs Bucket.
    Script to be run in the SecLog-Master account
    StackName should contain EC-Customer
    v1.1.5 Defines two lambda functions to push logs created by the linked accounts
    to its corresponding log group

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      -
        Label:
          default: "Tags"
        Parameters:
          - owner
          - project
          - environment
          - criticity
          - dataClassification
          - lambdaloglevel

Parameters:
    owner:
        Description: "Tag: owner. Creator or responsible for the project"
        Type: String
        Default: ''

    environment:
        Description: "Tag: environment. Environnement of the resource (Dev, test, prod etc.)"
        Type: String
        Default: ''

    criticity:
        Description: "Tag: criticity. Criticity of the application"
        Type: String
        Default: ''

    project:
        Description: "Tag: project. Name of the application or project"
        Type: String
        Default: 'Landing Zone'

    dataClassification:
        Description: "Tag: data-classification. Level of criticality of the stored data"
        Type: String
        Default: ''

    lambdaLogLevel:
        Description: 'Lambda logging level'
        Type: String
        Default: 'INFO'

Mappings: {}

Resources:

  
    LogShipperLambdaExecutionRole:
      Type: 'AWS::IAM::Role'
      Properties:
        AssumeRolePolicyDocument:
          Version: 2012-10-17
          Statement:
            - Effect: Allow
              Principal:
                Service:
                  - lambda.amazonaws.com
              Action:
                - 'sts:AssumeRole'
        Path: /
        Policies:
          - PolicyName: root
            PolicyDocument:
              Version: 2012-10-17
              Statement:
                - Effect: Allow
                  Action:
                    - 'logs:CreateLogGroup'
                    - 'logs:CreateLogStream'
                    - 'logs:DescribeLogStreams'
                  Resource: 
                    Fn::Join:
                      - ""
                      - - "arn:aws:logs:*:"
                        - Ref: AWS::AccountId
                        - ":log-group:*"
                - Effect: Allow
                  Action:
                    - 'logs:PutLogEvents'
                  Resource: 
                    Fn::Join:
                      - ""
                      - - "arn:aws:logs:*:"
                        - Ref: AWS::AccountId
                        - ":log-group:*:log-stream:*"
                - Effect: Allow
                  Action:
                  - 's3:GetObject'
                  Resource: 
                  - Fn::Join:
                    - ""
                    - - "arn:aws:s3:::cloudtrail-logs-"
                      - Ref: AWS::AccountId
                      - "-do-not-delete/*"
                  - Fn::Join:
                    - ""
                    - - "arn:aws:s3:::config-logs-"
                      - Ref: AWS::AccountId
                      - "-do-not-delete/*"
                - Effect: Allow
                  Action:
                  - 'kms:Decrypt'
                  Resource: 
                    Fn::Join:
                      - ""
                      - - "arn:aws:kms:*:"
                        - Ref: AWS::AccountId
                        - ":key/*"

    #   -------------------
    #   CloudTrail Bucket
    #   -------------------

    CloudTrailBucket:
        Type: AWS::S3::Bucket
        DependsOn:
          - CloudTraiLambdaPermission
        Properties:
            BucketName:
                Fn::Join:
                - ""
                - - "cloudtrail-logs-"
                  - Ref: AWS::AccountId
                  - "-do-not-delete"
            NotificationConfiguration:
              LambdaConfigurations:
                - Event: s3:ObjectCreated:*
                  Function: !GetAtt CloudTrailLogShipperFunction.Arn
            PublicAccessBlockConfiguration: 
              BlockPublicAcls: true
              BlockPublicPolicy: true
              IgnorePublicAcls: true
              RestrictPublicBuckets: true
            LifecycleConfiguration:
                Rules:
                 - 
                    Status: Enabled
                    ExpirationInDays: 425
                    NoncurrentVersionExpirationInDays: 425
                    Transitions:
                     - 
                        TransitionInDays: 60
                        StorageClass: GLACIER
                     - 
                        TransitionInDays: 30
                        StorageClass: STANDARD_IA

                    NoncurrentVersionTransitions:
                     - 
                        TransitionInDays: 60
                        StorageClass: GLACIER
                     - 
                        TransitionInDays: 30
                        StorageClass: STANDARD_IA

            LoggingConfiguration:
                DestinationBucketName: !Ref AccessLogsBucket
            VersioningConfiguration:
                Status: Enabled
            Tags:
            - Key: owner
              Value: !Ref owner
            - Key: environment
              Value: !Ref environment
            - Key: criticity
              Value: !Ref criticity
            - Key: project
              Value: !Ref project
            - Key: data-classification
              Value: !Ref dataClassification

    CloudTrailBucketPolicy:
        Type: AWS::S3::BucketPolicy
        Properties:
            Bucket:
                Ref: CloudTrailBucket
            PolicyDocument:
                Statement: 
                 - 
                   Sid: "CloudTrailBucketPolicyDelivery1"
                   Effect: "Allow"
                   Principal:
                    Service:
                     - "cloudtrail.amazonaws.com"
                   Action:
                    - "s3:GetBucketAcl"
                   Resource: !GetAtt CloudTrailBucket.Arn

                 - 
                   Sid: "CloudTrailBucketPolicyDelivery2"
                   Effect: "Allow"
                   Principal:
                    Service:
                     - "cloudtrail.amazonaws.com"
                   Action:
                    - "s3:PutObject"
                   Resource: 
                     Fn::Join: 
                       - ""
                       - 
                         - !GetAtt CloudTrailBucket.Arn
                         - "/*"
                   Condition:
                    StringEquals:
                        "s3:x-amz-acl": "bucket-owner-full-control"
                 -
                   Sid: "AWSCloudTrailgBucketSSL"
                   Action: s3:*
                   Principal:
                    Service: cloudtrail.amazonaws.com
                   Effect: Deny
                   Resource: 
                     Fn::Join: 
                       - ""
                       - 
                         - !GetAtt CloudTrailBucket.Arn
                         - "/*"
                   Condition:
                     Bool:
                       "aws:SecureTransport": "false"

    CloudTraiLambdaPermission:
      Type: AWS::Lambda::Permission
      Properties:
        Action: 'lambda:InvokeFunction'
        FunctionName: !Ref CloudTrailLogShipperFunction
        Principal: s3.amazonaws.com
        SourceArn:
          Fn::Join:
            - ""
            - - "arn:aws:s3:::cloudtrail-logs-"
              - Ref: AWS::AccountId
              - "-do-not-delete"
        SourceAccount: !Ref AWS::AccountId 

    CloudTrailLogShipperFunction:
      Type: 'AWS::Lambda::Function'
      Properties:
        Code:
          ZipFile: !Sub |
            import boto3
            import gzip
            import re
            import json
            import os
            import re
            import logging
            import time
            from io import BytesIO
            from datetime import datetime
            def lambda_handler(event, context):
              cloudwatch = boto3.client('logs')
              sts = boto3.client('sts')
              s3= boto3.client('s3')
              logger = logging.getLogger()
              logger.setLevel(getLoggingLevel(os.environ['LOG_LEVEL']))
              region = os.environ['AWS_REGION']
              # get the seclog account id
              account = sts.get_caller_identity()
              for record in event['Records']:
                try:
                  # get the file key from the S3 event
                  filename = record['s3']['object']['key']
                  logger.info('S3 object detected: ' + filename)
                  pattern = 'AWSLogs/\d+/CloudTrail/'
                  result = re.match(pattern, filename)
                  if not account['Account'] in filename and result: 
                    # retrieve bucket name and file_key from the S3 event
                    bucketname = record['s3']['bucket']['name']
                    logger.debug('S3 bucket: ' + bucketname)
                    # obtain logstream name
                    l = re.split(r'/',filename)
                    logstreamname = l[1] + '_CloudTrail_' + region
                    logger.debug('Linked account: : ' + l[1])                    
                    # get the object
                    obj = s3.get_object(Bucket=bucketname, Key=filename)
                    logger.debug('Retrieve S3 object')
                    # get the content as text
                    n = obj['Body'].read()
                    gzipfile = BytesIO(n)
                    gzipfile = gzip.GzipFile(fileobj=gzipfile)
                    content = gzipfile.read()
                    logger.debug('S3 object size: ' + str(len(content)))
                    dt_obj = datetime.strptime(record['eventTime'],'%Y-%m-%dT%H:%M:%S.%fZ')
                    ts = int(float(dt_obj.timestamp()) * 1000)
                    logger.debug('Timestamp for this event: ' + str(ts))
                    # check and create logstream
                    nextToken = '' 
                    dict = cloudwatch.describe_log_streams(
                      logGroupName='/aws/cloudtrail',
                      logStreamNamePrefix=logstreamname)
                    li = list(filter(lambda ls: ls['logStreamName'] == logstreamname, dict['logStreams']))
                    if li is None or len(li) == 0:
                      try:
                        cloudwatch.create_log_stream(
                          logGroupName='/aws/cloudtrail',
                          logStreamName=logstreamname)
                        logger.info('Log stream created: ' + logstreamname)
                      except Exception as e:
                        logger.error('Logstream already exists ' + logstreamname)
                        raise e
                    else:
                      logger.debug('Found logstream: ' + str(li))
                      if 'uploadSequenceToken' in li[0]:
                        nextToken = li[0]['uploadSequenceToken']
                        logger.debug('uploadSequenceToken: ' + nextToken)
                    json_content = json.loads(content.decode('utf-8'))
                    throttle = 0
                    for record in json_content['Records']:
                      if throttle % 5 == 0:
                        time.sleep(1)
                        logger.debug('throttling... ')
                      throttle +=1
                      if not nextToken:
                        result = cloudwatch.put_log_events(
                          logGroupName='/aws/cloudtrail',
                          logStreamName=logstreamname,
                          logEvents=[
                            {
                              'timestamp': ts,
                              'message': str(record)
                            },
                          ]
                        )
                      else:
                        result = cloudwatch.put_log_events(
                          logGroupName='/aws/cloudtrail',
                          logStreamName=logstreamname,
                          logEvents=[
                            {
                              'timestamp': ts,
                              'message': str(record)
                            },
                          ],
                          sequenceToken=nextToken
                        )
                      nextToken = result['nextSequenceToken']
                    logger.info(str(throttle) + ' log entries from S3 object created')
                except Exception as e:
                  logger.error('Log entry failed: ' + str(e))
                  raise e 
            def getLoggingLevel(a):
              if a == 'CRITICAL':
                return 50
              elif a == 'ERROR':
                return 40
              elif a == 'WARNING':
                return 30
              elif a == 'INFO':
                return 20
              elif a == 'DEBUG':
                return 10
              else:
                return 0          
        Handler: 'index.lambda_handler'
        MemorySize: 128
        ReservedConcurrentExecutions: 1
        Role: !GetAtt LogShipperLambdaExecutionRole.Arn
        Runtime: python3.6
        Timeout: 60
        Environment:
          Variables:
            LOG_LEVEL: !Ref lambdaLogLevel
  
    #   -------------------
    #   Config Bucket
    #   -------------------

    ConfigBucket:
        Type: AWS::S3::Bucket
        DependsOn:
          - ConfigLambdaPermission
        Properties:
            BucketName:
                Fn::Join:
                - ""
                - - "config-logs-"
                  - Ref: AWS::AccountId
                  - "-do-not-delete"
            NotificationConfiguration:
              LambdaConfigurations:
                - Event: s3:ObjectCreated:*
                  Function: !GetAtt ConfigLogShipperFunction.Arn
            LifecycleConfiguration:
                Rules:
                 - 
                    Status: Enabled
                    ExpirationInDays: 425
                    NoncurrentVersionExpirationInDays: 425
                    Transitions:
                     - 
                        TransitionInDays: 60
                        StorageClass: GLACIER
                     - 
                        TransitionInDays: 30
                        StorageClass: STANDARD_IA

                    NoncurrentVersionTransitions:
                     - 
                        TransitionInDays: 60
                        StorageClass: GLACIER
                     - 
                        TransitionInDays: 30
                        StorageClass: STANDARD_IA
            LoggingConfiguration:
                DestinationBucketName: !Ref AccessLogsBucket
            VersioningConfiguration:
                Status: Enabled
            PublicAccessBlockConfiguration: 
              BlockPublicAcls: true
              BlockPublicPolicy: true
              IgnorePublicAcls: true
              RestrictPublicBuckets: true
            Tags:
            - Key: owner
              Value: !Ref owner
            - Key: environment
              Value: !Ref environment
            - Key: criticity
              Value: !Ref criticity
            - Key: project
              Value: !Ref project
            - Key: data-classification
              Value: !Ref dataClassification

    ConfigBucketPolicy:
        Type: AWS::S3::BucketPolicy
        Properties:
            Bucket:
                Ref: ConfigBucket
            PolicyDocument:
                Statement: 
                 - 
                   Sid: "AWSConfigBucketPermissionsCheck"
                   Effect: "Allow"
                   Principal:
                    Service:
                     - "config.amazonaws.com"
                   Action:
                    - "s3:GetBucketAcl"
                   Resource: !GetAtt ConfigBucket.Arn

                 - 
                   Sid: "AWSConfigBucketDelivery"
                   Effect: "Allow"
                   Principal:
                    Service:
                     - "config.amazonaws.com"
                   Action:
                    - "s3:PutObject"
                   Resource: 
                     Fn::Join: 
                       - ""
                       - 
                         - !GetAtt ConfigBucket.Arn
                         - "/*"
                   Condition:
                    StringEquals:
                        "s3:x-amz-acl": "bucket-owner-full-control"
                 -
                   Sid: "AWSConfigBucketSSL"
                   Action: "s3:*"
                   Principal:
                    Service: config.amazonaws.com
                   Effect: Deny
                   Resource: 
                     Fn::Join: 
                       - ""
                       - 
                         - !GetAtt ConfigBucket.Arn
                         - "/*"
                   Condition:
                    Bool:
                      "aws:SecureTransport": "false"

    ConfigLambdaPermission:
      Type: AWS::Lambda::Permission
      Properties:
        Action: 'lambda:InvokeFunction'
        FunctionName: !Ref ConfigLogShipperFunction
        Principal: s3.amazonaws.com
        SourceArn:
          Fn::Join:
            - ""
            - - "arn:aws:s3:::config-logs-"
              - Ref: AWS::AccountId
              - "-do-not-delete"
        SourceAccount: !Ref AWS::AccountId  

    ConfigLogShipperFunction:
      Type: 'AWS::Lambda::Function'
      Properties:
        Code:
          ZipFile: !Sub |
            import boto3
            import gzip
            import re
            import json
            import os
            import re
            import logging
            import time
            from io import BytesIO
            from datetime import datetime
            def lambda_handler(event, context):
              cloudwatch = boto3.client('logs')
              sts = boto3.client('sts')
              s3= boto3.client('s3')
              logger = logging.getLogger()
              logger.setLevel(getLoggingLevel(os.environ['LOG_LEVEL']))
              region = os.environ['AWS_REGION']
              # get the seclog account id
              account = sts.get_caller_identity()
              for record in event['Records']:
                try:
                  # get the file key from the S3 event
                  filename = record['s3']['object']['key']
                  logger.info('S3 object detected: ' + filename)
                  pattern = 'AWSLogs/\d+/Config/'
                  result = re.match(pattern, filename)
                  if not account['Account'] in filename and result and 'ConfigWritabilityCheckFile' not in filename: 
                    # retrieve bucket name and file_key from the S3 event
                    bucketname = record['s3']['bucket']['name']
                    logger.debug('S3 bucket: ' + bucketname)
                    # obtain logstream name
                    l = re.split(r'/',filename)
                    logstreamname = l[1] + '_Config_' + region
                    logger.debug('Linked account: : ' + l[1])                    
                    # get the object
                    obj = s3.get_object(Bucket=bucketname, Key=filename)
                    logger.debug('Retrieve S3 object')
                    # get the content as text
                    n = obj['Body'].read()
                    gzipfile = BytesIO(n)
                    gzipfile = gzip.GzipFile(fileobj=gzipfile)
                    content = gzipfile.read()
                    logger.debug('S3 object size: ' + str(len(content)))
                    dt_obj = datetime.strptime(record['eventTime'],'%Y-%m-%dT%H:%M:%S.%fZ')
                    ts = int(float(dt_obj.timestamp()) * 1000)
                    logger.debug('Timestamp for this event: ' + str(ts))
                    # check and create logstream
                    nextToken = '' 
                    dict = cloudwatch.describe_log_streams(
                      logGroupName='/aws/events/config',
                      logStreamNamePrefix=logstreamname)
                    li = list(filter(lambda ls: ls['logStreamName'] == logstreamname, dict['logStreams']))
                    if li is None or len(li) == 0:
                      try:
                        cloudwatch.create_log_stream(
                          logGroupName='/aws/events/config',
                          logStreamName=logstreamname)
                        logger.info('Log stream created: ' + logstreamname)
                      except Exception as e:
                        logger.error('Logstream already exists ' + logstreamname)
                        raise e  
                    else:
                      logger.debug('Found logstream: ' + str(li))
                      if 'uploadSequenceToken' in li[0]:
                        nextToken = li[0]['uploadSequenceToken']
                        logger.debug('uploadSequenceToken: ' + nextToken)
                    json_content = json.loads(content.decode('utf-8'))
                    throttle = 0
                    for record in json_content['configurationItems']:              
                      if throttle % 5 == 0:
                        time.sleep(1)
                        logger.debug('throttling... ')
                      throttle +=1
                      if not nextToken:
                        result = cloudwatch.put_log_events(
                          logGroupName='/aws/events/config',
                          logStreamName=logstreamname,
                          logEvents=[
                            {
                              'timestamp': ts,
                              'message': str(record)
                            },
                          ]
                        )
                      else:
                        result = cloudwatch.put_log_events(
                          logGroupName='/aws/events/config',
                          logStreamName=logstreamname,
                          logEvents=[
                            {
                              'timestamp': ts,
                              'message': str(record)
                            },
                          ],
                          sequenceToken=nextToken
                        )
                      nextToken = result['nextSequenceToken']
                    logger.info(str(throttle) + ' log entries from S3 object created')
                except Exception as e:
                  logger.error('Log entry failed: ' + str(e))
                  raise e 
            def getLoggingLevel(a):
              if a == 'CRITICAL':
                return 50
              elif a == 'ERROR':
                return 40
              elif a == 'WARNING':
                return 30
              elif a == 'INFO':
                return 20
              elif a == 'DEBUG':
                return 10
              else:
                return 0
        Handler: 'index.lambda_handler'
        MemorySize: 128
        ReservedConcurrentExecutions: 1
        Role: !GetAtt LogShipperLambdaExecutionRole.Arn
        Runtime: python3.6
        Timeout: 60
        Environment:
          Variables:
            LOG_LEVEL: !Ref lambdaLogLevel 
    #   -------------------
    #   AccessLogs Bucket
    #   -------------------

    AccessLogsBucket:
        Type: AWS::S3::Bucket
        Properties:
          BucketName:
            Fn::Join:
            - ""
            - - "access-logs-"
              - Ref: AWS::AccountId
              - "-do-not-delete"
          AccessControl: LogDeliveryWrite
          Tags:
          - Key: owner
            Value: !Ref owner
          - Key: environment
            Value: !Ref environment
          - Key: criticity
            Value: !Ref criticity
          - Key: project
            Value: !Ref project
          - Key: data-classification
            Value: !Ref dataClassification
          PublicAccessBlockConfiguration: 
            BlockPublicAcls: true
            BlockPublicPolicy: true
            IgnorePublicAcls: true
            RestrictPublicBuckets: true

    AccessLogsPolicy:
        Type: AWS::S3::BucketPolicy
        Properties:
            Bucket:
                Ref: AccessLogsBucket
            PolicyDocument:
                Statement:                  
                 -
                   Sid: "AWSAccessLogsBucketSSL"
                   Action: "s3:*"
                   Effect: Deny
                   Principal:
                    Service: cloudwatch.amazonaws.com
                   Resource: 
                     Fn::Join: 
                       - ""
                       - 
                         - !GetAtt AccessLogsBucket.Arn
                         - "/*"
                   Condition:
                    Bool:
                      "aws:SecureTransport": "false"

Outputs:
  ConfigBucket:
    Description: Bucket for storing config-logs
    Value: !Ref ConfigBucket
    Export:
      Name: !Sub "${AWS::StackName}-ConfigBucket"

  CloudTrailBucket:
    Description: Bucket for storing cloudtrail-logs
    Value: !Ref CloudTrailBucket
    Export:
      Name: !Sub "${AWS::StackName}-CloudTrailBucket"

  AccessLogsBucket:
    Description: Bucket for storing access-logs
    Value: !Ref AccessLogsBucket
    Export:
      Name: !Sub "${AWS::StackName}-AccessLogsBucket"


